{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7255d378",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee86b5b3",
   "metadata": {},
   "source": [
    "Here we will merge two datasets: BoxOfficeMojo (boxofficemojo.com) and The Movie Database (TMDB). Here is what each contains:\n",
    "\n",
    "| Feature / Data Point                                  | **Box Office Mojo (BOM)** üèõ               | **TMDb (The Movie Database)** üé¨                       |\n",
    "| ----------------------------------------------------- | ------------------------------------------ | ------------------------------------------------------ |\n",
    "| **Domestic box office grosses**                       | ‚úÖ Accurate (daily/weekly/yearly US/Canada) | ‚ùå Not provided (only worldwide revenue, often missing) |\n",
    "| **Worldwide box office**                              | ‚ö†Ô∏è Limited / inconsistent                  | ‚úÖ Available (but often incomplete)                     |\n",
    "| **Theater counts**                                    | ‚úÖ Number of theaters per release           | ‚ùå Not available                                        |\n",
    "| **Budgets**                                           | ‚ùå Rarely included                          | ‚úÖ Included (when known)                                |\n",
    "| **Genres**                                            | ‚ùå Not available                            | ‚úÖ Rich genre metadata (IDs + names)                    |\n",
    "| **Languages**                                         | ‚ùå Not available                            | ‚úÖ Original language + spoken languages                 |\n",
    "| **Production companies/countries**                    | ‚ùå Not available                            | ‚úÖ Provided                                             |\n",
    "| **Movie metadata** (runtime, overview, posters, etc.) | ‚ùå Not available                            | ‚úÖ Extensive metadata                                   |\n",
    "| **Upcoming movies (future years)**                    | ‚ùå Only past releases                       | ‚úÖ Includes ‚ÄúIn Production‚Äù, ‚ÄúPlanned‚Äù, 2026+           |\n",
    "| **Filters** (e.g. exclude TV, docs, non-English)      | ‚ùå No filters                               | ‚úÖ Yes (via metadata)                                   |\n",
    "| **Coverage**                                          | ‚úÖ US theatrical releases only              | ‚úÖ Worldwide releases (films + TV)                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154d12e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ requests already installed\n",
      "üì¶ Installing beautifulsoup4...\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/miniconda3/envs/box_office/lib/python3.11/site-packages (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/miniconda3/envs/box_office/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/miniconda3/envs/box_office/lib/python3.11/site-packages (from beautifulsoup4) (4.12.2)\n",
      "‚úÖ beautifulsoup4 installed\n"
     ]
    }
   ],
   "source": [
    "# ---- 0) Setup / imports (install if missing) ----\n",
    "import subprocess, sys, os, time, json, requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "def _ensure(pkg, import_name=None):\n",
    "    try:\n",
    "        __import__(import_name or pkg.replace(\"-\", \"_\"))\n",
    "        print(f\"‚úÖ {pkg} already installed\")\n",
    "    except Exception:\n",
    "        print(f\"üì¶ Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "        print(f\"‚úÖ {pkg} installed\")\n",
    "\n",
    "for pkg in [\"requests\", \"beautifulsoup4\"]:\n",
    "    _ensure(pkg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf06c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Globals ----\n",
    "DATA_DIR = \"../data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "START_YEAR = 2015\n",
    "END_YEAR = 2026\n",
    "FORCE_REFRESH = False   # flip to True to re-scrape/re-fetch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae588ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2) Cache wrapper - so you don't have to rescrape every time ----\n",
    "def load_or_build_csv(path, builder_fn, *, force=FORCE_REFRESH, name=\"dataset\"):\n",
    "    \"\"\"\n",
    "    If `path` exists and not forcing, load CSV.\n",
    "    Otherwise, call `builder_fn()` -> DataFrame, save to CSV, return it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (not force) and os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "            print(f\"üóÇÔ∏è  Using cached {name}: {os.path.relpath(path)}\")\n",
    "            return pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Cache read issue for {name}: {e} ‚Äî rebuilding\")\n",
    "\n",
    "    print(f\"üîÑ Building {name} ‚Ä¶\")\n",
    "    df = builder_fn()\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"üíæ Saved {name} ‚Üí {os.path.relpath(path)}  ({len(df)} rows)\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0556964",
   "metadata": {},
   "source": [
    "### Box Office Mojo Scrape Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c8593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 3) Fetch All-Time Domestic Grosses (lifetime) [multi-endpoint + robust] ----\n",
    "import time, warnings\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_alltime_domestic(max_pages=50, sleep=0.35, per_page=200, retries=3, retry_sleep=1.0):\n",
    "    \"\"\"\n",
    "    Scrape the All-Time Domestic Grosses list from Box Office Mojo.\n",
    "    Tries multiple known endpoints and normalizes columns.\n",
    "\n",
    "    It will:\n",
    "      - Use a real User-Agent\n",
    "      - Retry each page\n",
    "      - Try these endpoints in order until one works:\n",
    "          1) /chart/domestic/\n",
    "          2) /chart/top_lifetime_gross/\n",
    "          3) /chart/top_lifetime_gross/?area=NA\n",
    "      - Paginate in 200-row increments\n",
    "    Returns one row per film with lifetime domestic gross.\n",
    "    \"\"\"\n",
    "    endpoints = [\n",
    "        \"https://www.boxofficemojo.com/chart/domestic/\",\n",
    "        \"https://www.boxofficemojo.com/chart/top_lifetime_gross/\",\n",
    "        \"https://www.boxofficemojo.com/chart/top_lifetime_gross/?area=NA\",\n",
    "    ]\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/127.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "\n",
    "    frames = []\n",
    "    used_base = None\n",
    "\n",
    "    # Try endpoints until one yields a valid table\n",
    "    for base in endpoints:\n",
    "        frames.clear()\n",
    "        used_base = base\n",
    "        offset = 0\n",
    "\n",
    "        for _ in range(max_pages):\n",
    "            url = f\"{base}&offset={offset}\" if (\"?\" in base and offset) else (f\"{base}?offset={offset}\" if offset else base)\n",
    "\n",
    "            # --- retry loop per page ---\n",
    "            last_exc = None\n",
    "            html = None\n",
    "            for attempt in range(1, retries + 1):\n",
    "                try:\n",
    "                    r = requests.get(url, headers=headers, timeout=20)\n",
    "                    # look for the mojo table marker or any table at all\n",
    "                    if r.status_code == 200 and r.text and (\"mojo-body-table\" in r.text or \"<table\" in r.text):\n",
    "                        html = r.text\n",
    "                        break\n",
    "                    else:\n",
    "                        last_exc = RuntimeError(f\"HTTP {r.status_code} / unexpected content\")\n",
    "                except Exception as e:\n",
    "                    last_exc = e\n",
    "                time.sleep(retry_sleep)\n",
    "\n",
    "            if html is None:\n",
    "                # stop this endpoint if first page fails; try next endpoint\n",
    "                if offset == 0:\n",
    "                    frames.clear()\n",
    "                break\n",
    "\n",
    "            # --- parse tables safely ---\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            table = soup.select_one(\"table.a-bordered.a-horizontal-stripes.mojo-body-table\")\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                if table is not None:\n",
    "                    tables = pd.read_html(str(table))\n",
    "                else:\n",
    "                    tables = pd.read_html(html)\n",
    "\n",
    "            # Choose a table that has Title + a gross column + Year/Rank likely present\n",
    "            def _norm_cols(df):\n",
    "                return [str(c).strip() for c in df.columns]\n",
    "\n",
    "            candidates = []\n",
    "            for t in tables:\n",
    "                t.columns = _norm_cols(t)\n",
    "                cols = set([c.lower() for c in t.columns])\n",
    "                if \"title\" in cols and (\"lifetime gross\" in cols or \"gross\" in cols) and (\"year\" in cols or \"release year\" in cols or \"rank\" in cols):\n",
    "                    candidates.append(t)\n",
    "\n",
    "            if not candidates:\n",
    "                break\n",
    "\n",
    "            df = candidates[0]\n",
    "            if df.empty:\n",
    "                break\n",
    "\n",
    "            frames.append(df)\n",
    "\n",
    "            # pagination: 200-row pages typically\n",
    "            if len(df) < per_page:\n",
    "                break\n",
    "            offset += per_page\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        if frames:\n",
    "            # success on this endpoint\n",
    "            break\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"No data scraped from Box Office Mojo across all endpoints\")\n",
    "\n",
    "    alltime = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # --- Standardize & clean ---\n",
    "    # Normalize column labels seen across endpoints\n",
    "    rename_candidates = {\n",
    "        \"Title\": \"title\",\n",
    "        \"Lifetime Gross\": \"domestic_revenue\",\n",
    "        \"Gross\": \"domestic_revenue\",          # used on top_lifetime_gross pages\n",
    "        \"Year\": \"release_year\",\n",
    "        \"Release Year\": \"release_year\",\n",
    "        \"Rank\": \"rank\",\n",
    "    }\n",
    "    # First trim whitespace in columns\n",
    "    alltime.columns = [str(c).strip() for c in alltime.columns]\n",
    "    # Then rename if we recognize\n",
    "    for k, v in list(rename_candidates.items()):\n",
    "        if k in alltime.columns:\n",
    "            alltime = alltime.rename(columns={k: v})\n",
    "\n",
    "    # Ensure needed columns exist\n",
    "    for col in [\"title\", \"domestic_revenue\", \"release_year\", \"rank\"]:\n",
    "        if col not in alltime.columns:\n",
    "            alltime[col] = np.nan\n",
    "\n",
    "    # Clean strings/numbers\n",
    "    alltime[\"title\"] = alltime[\"title\"].astype(str).str.strip()\n",
    "    alltime[\"domestic_revenue\"] = alltime[\"domestic_revenue\"].astype(str).str.replace(r\"[\\$,]\", \"\", regex=True)\n",
    "    alltime[\"domestic_revenue\"] = pd.to_numeric(alltime[\"domestic_revenue\"], errors=\"coerce\")\n",
    "    alltime[\"release_year\"] = pd.to_numeric(alltime[\"release_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    alltime[\"rank\"] = pd.to_numeric(alltime[\"rank\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Drop any clearly invalid rows (header echoes, etc.)\n",
    "    alltime = alltime.dropna(subset=[\"title\", \"domestic_revenue\"], how=\"any\")\n",
    "\n",
    "    print(f\"üìä All-Time Domestic dataset ready from {used_base} : {alltime.shape}\")\n",
    "    return alltime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a27ac",
   "metadata": {},
   "source": [
    "### TMDB Scrape Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7343a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- 4) Fetch TMDb (v3/v4) ----\n",
    "def fetch_tmdb_movies(api_key, start_year=2015, end_year=2026,\n",
    "                      include_upcoming_pass=True, max_pages_per_year=5,\n",
    "                      region_us=True, min_vote_count=0, sleep_sec=0.25):\n",
    "    \"\"\"\n",
    "    Fetch movies from TMDb API (v3 or v4).\n",
    "    Falls back to static CSV if nothing is returned.\n",
    "    \"\"\"\n",
    "    if api_key is None or api_key == \"\" or api_key == \"YOUR_TMDB_API_KEY_HERE\":\n",
    "        print(\"‚ö†Ô∏è No TMDB key provided ‚Äî using fallback CSV\")\n",
    "        tmdb_df = pd.read_csv(os.path.join(DATA_DIR, \"TMDB_movie_dataset_v11.csv\"))\n",
    "        tmdb_df[\"release_date\"] = pd.to_datetime(tmdb_df[\"release_date\"], errors=\"coerce\")\n",
    "        tmdb_df[\"release_year\"] = tmdb_df[\"release_date\"].dt.year\n",
    "        return tmdb_df\n",
    "\n",
    "    is_v4 = str(api_key).startswith(\"eyJ\")  # v4 tokens look like JWTs\n",
    "    base_url = \"https://api.themoviedb.org/3\"\n",
    "    headers = {\"accept\": \"application/json\"}\n",
    "    if is_v4:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "\n",
    "    all_movies = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for page in range(1, max_pages_per_year + 1):\n",
    "            params = {\n",
    "                \"primary_release_year\": year,\n",
    "                \"page\": page,\n",
    "                \"language\": \"en-US\",\n",
    "                \"include_adult\": \"false\"\n",
    "            }\n",
    "            if region_us:\n",
    "                params[\"region\"] = \"US\"\n",
    "            if min_vote_count > 0:\n",
    "                params[\"vote_count.gte\"] = min_vote_count\n",
    "            if not is_v4:  # v3 key\n",
    "                params[\"api_key\"] = api_key\n",
    "\n",
    "            try:\n",
    "                r = requests.get(f\"{base_url}/discover/movie\", headers=headers, params=params, timeout=20)\n",
    "                if r.status_code != 200:\n",
    "                    print(f\"    Error {r.status_code} year={year} page={page}: {r.text[:200]}\")\n",
    "                    break\n",
    "                data = r.json()\n",
    "                all_movies.extend(data.get(\"results\", []))\n",
    "                if page >= data.get(\"total_pages\", 1):\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"    Request failed year={year} page={page}: {e}\")\n",
    "                break\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    if not all_movies:\n",
    "        raise RuntimeError(\"No movies fetched from TMDb\")\n",
    "\n",
    "    df = pd.DataFrame(all_movies)\n",
    "    if \"release_date\" in df.columns:\n",
    "        df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")\n",
    "        df[\"release_year\"] = df[\"release_date\"].dt.year\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6319810",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "Note you should have a config.json file with TMDB_API_KEY specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31748bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è  Using cached All-Time Domestic (lifetime): ../data/boxoffice_alltime_domestic.csv\n",
      "üîÑ Building TMDb (filtered) ‚Ä¶\n",
      "üîë Loaded TMDb key from /Users/jasmineplows/Documents/California/Projects/box_office/code/../config.json\n"
     ]
    }
   ],
   "source": [
    "# ---- 5) Load or build datasets with cache ----\n",
    "ALLTIME_CSV = os.path.join(DATA_DIR, \"boxoffice_alltime_domestic.csv\")\n",
    "TMDB_CSV   = os.path.join(DATA_DIR, \"tmdb_filtered.csv\")\n",
    "\n",
    "domestic_df = load_or_build_csv(\n",
    "    ALLTIME_CSV,\n",
    "    builder_fn=fetch_alltime_domestic,\n",
    "    name=\"All-Time Domestic (lifetime)\"\n",
    ")\n",
    "\n",
    "def load_tmdb_key():\n",
    "    \"\"\"\n",
    "    Load TMDb key from config.json (preferred) or environment.\n",
    "    Supports both v3 and v4 tokens.\n",
    "    \"\"\"\n",
    "    key = None\n",
    "    # search config.json up to 3 parent dirs\n",
    "    for up in [\"\", \"..\", \"../..\", \"../../..\"]:\n",
    "        cfg_path = os.path.join(os.getcwd(), up, \"config.json\")\n",
    "        if os.path.exists(cfg_path):\n",
    "            try:\n",
    "                with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    cfg = json.load(f)\n",
    "                key = cfg.get(\"TMDB_V4_TOKEN\") or cfg.get(\"TMDB_API_KEY\")\n",
    "                if key:\n",
    "                    print(f\"üîë Loaded TMDb key from {cfg_path}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not parse {cfg_path}: {e}\")\n",
    "\n",
    "    if not key:\n",
    "        key = os.getenv(\"TMDB_V4_TOKEN\") or os.getenv(\"TMDB_API_KEY\")\n",
    "\n",
    "    if not key:\n",
    "        raise RuntimeError(\"‚ùå No TMDB API key found in config.json or environment!\")\n",
    "\n",
    "    return key.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "\n",
    "def build_tmdb_filtered():\n",
    "    TMDB_API_KEY = load_tmdb_key()\n",
    "    return fetch_tmdb_movies(\n",
    "        TMDB_API_KEY,\n",
    "        start_year=START_YEAR,\n",
    "        end_year=END_YEAR,\n",
    "        include_upcoming_pass=False,\n",
    "        max_pages_per_year=100,\n",
    "        region_us=True,\n",
    "        min_vote_count=0,\n",
    "        sleep_sec=0.2\n",
    "    )\n",
    "\n",
    "tmdb_df = load_or_build_csv(\n",
    "    TMDB_CSV,\n",
    "    builder_fn=build_tmdb_filtered,\n",
    "    name=\"TMDb (filtered)\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Domestic: {domestic_df.shape}, TMDb: {tmdb_df.shape}\")\n",
    "display(domestic_df.head())\n",
    "display(tmdb_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dbec9a",
   "metadata": {},
   "source": [
    "### Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c982425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6) Merge TMDb with All-Time Domestic (exact + fuzzy)\n",
    "# ============================================\n",
    "\n",
    "# Install rapidfuzz if needed (for fuzzy merge fallback)\n",
    "import subprocess, sys, warnings\n",
    "try:\n",
    "    from rapidfuzz import process, fuzz\n",
    "except Exception:\n",
    "    print(\"üì¶ Installing rapidfuzz‚Ä¶\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rapidfuzz\"])\n",
    "    from rapidfuzz import process, fuzz\n",
    "\n",
    "def normalize_title(title: str) -> str:\n",
    "    if pd.isna(title): \n",
    "        return title\n",
    "    t = str(title).strip()\n",
    "    # Common subtitle noise (Star Wars episodes etc.)\n",
    "    for ep in [\"Episode I - \",\"Episode II - \",\"Episode III - \",\n",
    "               \"Episode IV - \",\"Episode V - \",\"Episode VI - \",\n",
    "               \"Episode VII - \",\"Episode VIII - \",\"Episode IX - \"]:\n",
    "        t = t.replace(ep, \"\")\n",
    "    t = t.replace(\" & \", \" and \")\n",
    "    t = \" \".join(t.split())\n",
    "    return t\n",
    "\n",
    "# ---- Clean/standardize Domestic (lifetime) ----\n",
    "domestic_clean = domestic_df.copy()\n",
    "# Ensure required columns exist\n",
    "for col in [\"title\", \"release_year\", \"domestic_revenue\", \"rank\"]:\n",
    "    if col not in domestic_clean.columns:\n",
    "        domestic_clean[col] = np.nan\n",
    "\n",
    "# Normalize\n",
    "domestic_clean[\"title\"] = domestic_clean[\"title\"].astype(str).str.strip()\n",
    "domestic_clean[\"title_normalized\"] = domestic_clean[\"title\"].apply(normalize_title)\n",
    "domestic_clean[\"release_year\"] = pd.to_numeric(domestic_clean[\"release_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "domestic_clean[\"domestic_revenue\"] = pd.to_numeric(domestic_clean[\"domestic_revenue\"], errors=\"coerce\")\n",
    "\n",
    "# Collapse to one row per (title_normalized, release_year)\n",
    "domestic_keyed = (\n",
    "    domestic_clean\n",
    "    .dropna(subset=[\"title_normalized\", \"release_year\"])\n",
    "    .groupby([\"title_normalized\", \"release_year\"], as_index=False)\n",
    "    .agg({\n",
    "        \"title\": \"first\",\n",
    "        \"domestic_revenue\": \"max\",\n",
    "        \"rank\": \"min\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# ---- Clean/standardize TMDb ----\n",
    "tmdb_clean = tmdb_df.copy()\n",
    "\n",
    "# Dates/years\n",
    "if \"release_date\" in tmdb_clean.columns:\n",
    "    tmdb_clean[\"release_date\"] = pd.to_datetime(tmdb_clean[\"release_date\"], errors=\"coerce\")\n",
    "    tmdb_clean[\"release_year\"] = tmdb_clean[\"release_date\"].dt.year.where(\n",
    "        tmdb_clean.get(\"release_year\").isna() if \"release_year\" in tmdb_clean.columns else True,\n",
    "        tmdb_clean.get(\"release_year\")\n",
    "    )\n",
    "tmdb_clean[\"release_year\"] = pd.to_numeric(tmdb_clean[\"release_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Language + genre filters (English, exclude Documentary=99, TV Movie=10770)\n",
    "tmdb_clean[\"original_language\"] = tmdb_clean.get(\"original_language\", \"en\").fillna(\"en\")\n",
    "tmdb_clean = tmdb_clean[tmdb_clean[\"original_language\"] == \"en\"]\n",
    "\n",
    "# Create a uniform \"genres\" string if we just have genre_ids\n",
    "if \"genres\" not in tmdb_clean.columns:\n",
    "    if \"genre_ids\" in tmdb_clean.columns:\n",
    "        tmdb_clean[\"genres\"] = tmdb_clean[\"genre_ids\"].astype(str)\n",
    "    else:\n",
    "        tmdb_clean[\"genres\"] = \"\"\n",
    "\n",
    "# Exclude docs & TV movies wherever possible\n",
    "def _contains_genre_ids_as_text(s, ids=(\"99\", \"10770\")):\n",
    "    # conservative check on the stringified list e.g. \"[28, 12, 99]\"\n",
    "    st = str(s)\n",
    "    return any(f\"{gid}\" in st for gid in ids)\n",
    "\n",
    "mask_exclude = tmdb_clean[\"genres\"].apply(_contains_genre_ids_as_text)\n",
    "tmdb_clean = tmdb_clean[~mask_exclude].copy()\n",
    "\n",
    "# Normalize title\n",
    "tmdb_clean[\"title\"] = tmdb_clean[\"title\"].astype(str).str.strip()\n",
    "tmdb_clean[\"title_normalized\"] = tmdb_clean[\"title\"].apply(normalize_title)\n",
    "\n",
    "# Collapse TMDb to one row per (title_normalized, release_year) preferring \"prominent\" entries\n",
    "tmdb_keyed = (\n",
    "    tmdb_clean.sort_values([\"release_year\",\"vote_count\",\"popularity\"], ascending=[True, False, False])\n",
    "              .drop_duplicates(subset=[\"title_normalized\",\"release_year\"], keep=\"first\")\n",
    ")\n",
    "\n",
    "print(\"üîó Exact merge TMDb ‚ü∑ Domestic (lifetime)‚Ä¶\")\n",
    "merged_df = pd.merge(\n",
    "    tmdb_keyed,\n",
    "    domestic_keyed[[\"title_normalized\",\"release_year\",\"domestic_revenue\",\"rank\"]],\n",
    "    on=[\"title_normalized\",\"release_year\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_domestic\"),\n",
    ")\n",
    "exact_hits = merged_df[\"domestic_revenue\"].notna().sum()\n",
    "print(f\"‚úÖ Exact matches: {exact_hits:,}\")\n",
    "\n",
    "# ---------- Fuzzy fallback (same-year only) ----------\n",
    "def fuzzy_fill_domestic(merged, domestic, score_cutoff=90):\n",
    "    \"\"\"\n",
    "    For rows in `merged` missing domestic_revenue, fuzzy-match titles within the same release_year.\n",
    "    Transfers domestic_revenue (+ rank).\n",
    "    \"\"\"\n",
    "    # Build per-year lookups\n",
    "    dom_by_year = {}\n",
    "    dom = domestic[[\"title_normalized\", \"release_year\", \"domestic_revenue\", \"rank\"]].dropna(subset=[\"title_normalized\"])\n",
    "    for y, sub in dom.groupby(\"release_year\"):\n",
    "        dom_by_year[int(y)] = (sub[\"title_normalized\"].tolist(), sub.index.tolist())\n",
    "\n",
    "    added = 0\n",
    "    missing_mask = merged[\"domestic_revenue\"].isna()\n",
    "    groups = merged[missing_mask].groupby(\"release_year\").groups\n",
    "\n",
    "    for y, idxs in groups.items():\n",
    "        if pd.isna(y):\n",
    "            continue\n",
    "        y = int(y)\n",
    "        if y not in dom_by_year:\n",
    "            continue\n",
    "        titles_dom, idxs_dom = dom_by_year[y]\n",
    "        if not titles_dom:\n",
    "            continue\n",
    "\n",
    "        for ridx in idxs:\n",
    "            q = merged.at[ridx, \"title_normalized\"]\n",
    "            if not isinstance(q, str) or not q:\n",
    "                continue\n",
    "            match = process.extractOne(q, titles_dom, scorer=fuzz.WRatio, score_cutoff=score_cutoff)\n",
    "            if not match:\n",
    "                continue\n",
    "            _, score, pos = match\n",
    "            dom_idx = idxs_dom[pos]\n",
    "            merged.at[ridx, \"domestic_revenue\"] = domestic.loc[dom_idx, \"domestic_revenue\"]\n",
    "            merged.at[ridx, \"rank\"] = domestic.loc[dom_idx, \"rank\"]\n",
    "            added += 1\n",
    "    return merged, added\n",
    "\n",
    "print(\"üß™ Fuzzy matching unmatched rows (same year)‚Ä¶\")\n",
    "merged_df, fuzzy_added = fuzzy_fill_domestic(merged_df, domestic_keyed, score_cutoff=90)\n",
    "print(f\"‚ûï Fuzzy matches added: {fuzzy_added:,}\")\n",
    "\n",
    "# Final safety: drop any remaining duplicates on (title, release_year) keeping highest domestic\n",
    "if merged_df.duplicated(subset=[\"title\",\"release_year\"], keep=False).any():\n",
    "    merged_df = (merged_df\n",
    "        .sort_values([\"release_year\",\"domestic_revenue\"], ascending=[True, False])\n",
    "        .drop_duplicates(subset=[\"title\",\"release_year\"], keep=\"first\"))\n",
    "\n",
    "# Canonical revenue columns\n",
    "merged_df[\"revenue_domestic\"] = pd.to_numeric(merged_df[\"domestic_revenue\"], errors=\"coerce\")\n",
    "merged_df[\"revenue\"] = merged_df[\"revenue_domestic\"]  # your modeling target = lifetime domestic\n",
    "# (Optionally keep worldwide if present from TMDb ‚Äî often incomplete)\n",
    "if \"revenue\" in tmdb_df.columns and \"revenue_worldwide\" not in merged_df.columns:\n",
    "    merged_df[\"revenue_worldwide\"] = pd.to_numeric(merged_df[\"revenue\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"‚úÖ Merge complete. Rows: {len(merged_df):,}\")\n",
    "display(merged_df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d37e9fa",
   "metadata": {},
   "source": [
    "### Filter + Export\n",
    "Now, go to `2.feature-engineering.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4bee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7) Final filters + save + summary\n",
    "# ============================================\n",
    "\n",
    "final_df = merged_df.copy()\n",
    "\n",
    "# Keep modeling window\n",
    "final_df = final_df[final_df[\"release_year\"].between(START_YEAR, END_YEAR)]\n",
    "\n",
    "# Keep rows with known lifetime domestic revenue\n",
    "final_df = final_df[final_df[\"revenue_domestic\"].notna() & (final_df[\"revenue_domestic\"] > 0)]\n",
    "\n",
    "# English-only\n",
    "if \"original_language\" in final_df.columns:\n",
    "    final_df = final_df[final_df[\"original_language\"] == \"en\"]\n",
    "\n",
    "# Drop TV/docs\n",
    "if \"genres\" in final_df.columns:\n",
    "    final_df = final_df[~final_df[\"genres\"].astype(str).str.contains(\"99|10770|Documentary|TV\", case=False, na=False)]\n",
    "\n",
    "# Save merged dataset (cached output)\n",
    "MERGED_CSV = os.path.join(DATA_DIR, \"dataset_domestic_lifetime_merged.csv\")\n",
    "final_df.to_csv(MERGED_CSV, index=False)\n",
    "print(f\"\\nüíæ Saved merged dataset ‚Üí {MERGED_CSV}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"Total movies: {len(final_df):,}\")\n",
    "if len(final_df) > 0:\n",
    "    yr_min = int(final_df[\"release_year\"].min())\n",
    "    yr_max = int(final_df[\"release_year\"].max())\n",
    "    rev_min = final_df[\"revenue_domestic\"].min()\n",
    "    rev_max = final_df[\"revenue_domestic\"].max()\n",
    "    rev_avg = final_df[\"revenue_domestic\"].mean()\n",
    "\n",
    "    print(f\"Year range: {yr_min}‚Äì{yr_max}\")\n",
    "    print(f\"Lifetime domestic range: ${rev_min:,.0f} ‚Äî ${rev_max:,.0f}\")\n",
    "    print(f\"Average lifetime domestic: ${rev_avg:,.0f}\")\n",
    "\n",
    "    display(\n",
    "        final_df.nlargest(10, \"revenue_domestic\")[[\"title\",\"release_year\",\"revenue_domestic\"]]\n",
    "                .rename(columns={\"revenue_domestic\":\"lifetime_domestic\"})\n",
    "                .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Final dataset is empty ‚Äî review filters and merge keys.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "box_office",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
