{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# GPT-4o Feature Validation for Box Office Dataset\n",
    "\n",
    "This notebook validates and corrects movie features using GPT-4o's knowledge. It sends movie information to the model and asks it to verify the accuracy of our engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wj0jmhkkbzb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ DATASET CONFIGURATION:\n",
      "==================================================\n",
      "üìä Dataset Configuration: English Only\n",
      "   Description: English movies only\n",
      "   Year range: 2015-2026\n",
      "   Training file: dataset_domestic_processed_english_2015_2026.csv\n",
      "   Full file: dataset_domestic_processed_english_2015_2026.csv\n",
      "\n",
      "\n",
      "üí° To change scope, uncomment one of the CURRENT_CONFIG lines above and re-run this cell\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATASET CONFIGURATION - Easy switching between dataset subsets\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path('../').resolve()))\n",
    "\n",
    "from dataset_config import (\n",
    "    DEFAULT_CONFIG, get_dataset_config, get_dataset_path, get_config_summary,\n",
    "    use_full_dataset, use_english_only, use_major_studios, use_english_major\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# CHOOSE YOUR DATASET SCOPE - Uncomment one line to switch\n",
    "# =============================================================================\n",
    "\n",
    "# CURRENT_CONFIG = use_full_dataset()                    # All studios, all languages (2010-2026)\n",
    "# CURRENT_CONFIG = use_english_only(2010)                # English only (2010-2026)\n",
    "CURRENT_CONFIG = use_english_only(2015)                # English only (2015-2026)\n",
    "# CURRENT_CONFIG = use_major_studios(2010)               # Major studios only (2010-2026)\n",
    "# CURRENT_CONFIG = use_major_studios(2015)               # Major studios only (2015-2026)\n",
    "# CURRENT_CONFIG = use_english_major(2010)               # English + Major studios (2010-2026)\n",
    "# CURRENT_CONFIG = use_english_major(2015)               # English + Major studios (2015-2026)\n",
    "\n",
    "# CURRENT_CONFIG = DEFAULT_CONFIG  # Use default (full dataset)\n",
    "\n",
    "print(\"üéØ DATASET CONFIGURATION:\")\n",
    "print(\"=\" * 50)\n",
    "print(get_config_summary(CURRENT_CONFIG))\n",
    "print(\"\\nüí° To change scope, uncomment one of the CURRENT_CONFIG lines above and re-run this cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API key loaded from config.json\n",
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OpenAI API setup\n",
    "import openai\n",
    "\n",
    "# Load API key from config.json\n",
    "try:\n",
    "    with open('../config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Set OpenAI API key\n",
    "    openai.api_key = config['OPENAI_API_KEY']\n",
    "    os.environ['OPENAI_API_KEY'] = config['OPENAI_API_KEY']\n",
    "\n",
    "    print(\"‚úÖ OpenAI API key loaded from config.json\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå config.json not found. Please create it with your OPENAI_API_KEY\")\n",
    "    print(\"Example config.json structure:\")\n",
    "    print('{\\n  \"OPENAI_API_KEY\": \"your-api-key-here\"\\n}')\n",
    "except KeyError:\n",
    "    print(\"‚ùå OPENAI_API_KEY not found in config.json\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading config: {e}\")\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Load Dataset and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "xvmxj3gxidh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading dataset: ../data/dataset_domestic_processed_english_2015_2026.csv\n",
      "No title corrections needed\n",
      "   ‚úÖ Loaded 1,307 movies\n",
      "   Year range: 2015-2026\n",
      "   Training (‚â§2023): 1,003 movies\n",
      "   Testing (2024): 145 movies\n",
      "   Evaluation (2025): 114 movies\n",
      "   Prediction (2026): 45 movies\n"
     ]
    }
   ],
   "source": [
    "# Load dataset using configuration system\n",
    "def load_dataset(training=False):\n",
    "    \"\"\"Load the configured dataset subset.\"\"\"\n",
    "    import pandas as pd\n",
    "    from movie_lists import normalize_domestic_titles\n",
    "\n",
    "    dataset_path = get_dataset_path(training=training, config=CURRENT_CONFIG)\n",
    "    dataset_config = get_dataset_config(CURRENT_CONFIG)\n",
    "\n",
    "    print(f\"üìÅ Loading dataset: {dataset_path}\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Apply additional filtering if needed (for english_major scope)\n",
    "    if dataset_config['scope'] == 'english_major':\n",
    "        if 'is_major_studio' in df.columns:\n",
    "            original_len = len(df)\n",
    "            df = df[df['is_major_studio'] == 1].copy()\n",
    "            print(f\"   Filtered to major studios: {len(df):,} movies (removed {original_len - len(df):,})\")\n",
    "\n",
    "    # Normalize titles\n",
    "    df = normalize_domestic_titles(df)\n",
    "\n",
    "    print(f\"   ‚úÖ Loaded {len(df):,} movies\")\n",
    "    if 'release_year' in df.columns:\n",
    "        print(f\"   Year range: {df['release_year'].min()}-{df['release_year'].max()}\")\n",
    "\n",
    "        # Show breakdown by time period\n",
    "        training_count = len(df[df['release_year'] <= 2023])\n",
    "        test_2024_count = len(df[df['release_year'] == 2024])\n",
    "        eval_2025_count = len(df[df['release_year'] == 2025])\n",
    "        pred_2026_count = len(df[df['release_year'] == 2026])\n",
    "\n",
    "        print(f\"   Training (‚â§2023): {training_count:,} movies\")\n",
    "        if test_2024_count > 0:\n",
    "            print(f\"   Testing (2024): {test_2024_count:,} movies\")\n",
    "        if eval_2025_count > 0:\n",
    "            print(f\"   Evaluation (2025): {eval_2025_count:,} movies\")\n",
    "        if pred_2026_count > 0:\n",
    "            print(f\"   Prediction (2026): {pred_2026_count:,} movies\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = load_dataset(training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03903554",
   "metadata": {},
   "source": [
    "## GPT-4o Validation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation functions defined with ALL 34 specified features\n"
     ]
    }
   ],
   "source": [
    "def create_validation_prompt(movie_data):\n",
    "    \"\"\"\n",
    "    Create a prompt for GPT-4o to validate movie features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract ALL specified features for validation (34 columns total)\n",
    "    feature_dict = {\n",
    "        # Basic movie info\n",
    "        'title': movie_data['title'],\n",
    "        'title_normalized': movie_data['title_normalized'],\n",
    "        'release_year': int(movie_data['release_year']),\n",
    "        \n",
    "        # Production/Origin info\n",
    "        'production_country_codes': str(movie_data['production_country_codes']),\n",
    "        'is_origin_usa': bool(movie_data['is_origin_usa']),\n",
    "        'is_origin_uk_ie': bool(movie_data['is_origin_uk_ie']),\n",
    "        'is_origin_canada': bool(movie_data['is_origin_canada']),\n",
    "        'is_origin_us_uk_ca': bool(movie_data['is_origin_us_uk_ca']),\n",
    "        \n",
    "        # Distribution\n",
    "        'distributor': str(movie_data['distributor']),\n",
    "        \n",
    "        # Studio features (9 total)\n",
    "        'is_disney': bool(movie_data['is_disney']),\n",
    "        'is_warner_bros': bool(movie_data['is_warner_bros']),\n",
    "        'is_universal': bool(movie_data['is_universal']),\n",
    "        'is_sony': bool(movie_data['is_sony']),\n",
    "        'is_paramount': bool(movie_data['is_paramount']),\n",
    "        'is_fox': bool(movie_data['is_fox']),\n",
    "        'is_mgm': bool(movie_data['is_mgm']),\n",
    "        'is_lionsgate': bool(movie_data['is_lionsgate']),\n",
    "        'is_major_studio': bool(movie_data['is_major_studio']),\n",
    "        \n",
    "        # Language\n",
    "        'is_english': bool(movie_data['is_english']),\n",
    "        \n",
    "        # IP and franchise features (15 total)\n",
    "        'is_sequel_title': bool(movie_data['is_sequel_title']),\n",
    "        'is_marvel': bool(movie_data['is_marvel']),\n",
    "        'is_dc': bool(movie_data['is_dc']),\n",
    "        'is_star_wars': bool(movie_data['is_star_wars']),\n",
    "        'is_fast_furious': bool(movie_data['is_fast_furious']),\n",
    "        'is_harry_potter': bool(movie_data['is_harry_potter']),\n",
    "        'is_franchise_sequel': bool(movie_data['is_franchise_sequel']),\n",
    "        'is_sequel': bool(movie_data['is_sequel']),\n",
    "        'is_live_action_remake': bool(movie_data['is_live_action_remake']),\n",
    "        'is_adaptation': bool(movie_data['is_adaptation']),\n",
    "        'is_superhero': bool(movie_data['is_superhero']),\n",
    "        'has_remake_indicator': bool(movie_data['has_remake_indicator']),\n",
    "        'is_remake_adaptation': bool(movie_data['is_remake_adaptation']),\n",
    "        'is_ip_movie': bool(movie_data['is_ip_movie'])\n",
    "    }\n",
    "    \n",
    "    # Convert feature dict to JSON string to avoid f-string nesting issues\n",
    "    feature_json = json.dumps(feature_dict, indent=2)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a movie expert tasked with validating feature classifications for a box office prediction model. \n",
    "\n",
    "Please review the following movie and its current feature classifications, then provide corrections for any inaccurate features.\n",
    "\n",
    "**Movie Information:**\n",
    "{feature_json}\n",
    "\n",
    "**Feature Definitions:**\n",
    "- title: Original movie title\n",
    "- title_normalized: Cleaned version of the title\n",
    "- release_year: Year the movie was released\n",
    "- production_country_codes: Country codes where movie was produced\n",
    "- is_origin_usa/uk_ie/canada/us_uk_ca: Movie origin country flags\n",
    "- distributor: Studio/company that distributed the movie\n",
    "- Studio flags (is_disney, is_warner_bros, etc.): Which major studio distributed it\n",
    "- is_major_studio: Whether distributed by a major studio (Disney, Warner Bros, Universal, Sony, Paramount, Fox, MGM, Lionsgate)\n",
    "- is_english: Whether the movie is in English language\n",
    "- is_sequel_title: Title contains sequel indicators (2, 3, Part, Chapter, etc.)\n",
    "- is_marvel: Marvel Cinematic Universe or Marvel Comics adaptation\n",
    "- is_dc: DC Comics adaptation\n",
    "- is_star_wars: Part of Star Wars franchise\n",
    "- is_fast_furious: Part of Fast & Furious franchise\n",
    "- is_harry_potter: Part of Harry Potter/Wizarding World\n",
    "- is_franchise_sequel: Sequel/spinoff of an established franchise\n",
    "- is_sequel: Any type of sequel (combines multiple sequel indicators)\n",
    "- is_live_action_remake: Live-action version of animated film or remake of classic\n",
    "- is_adaptation: Adapted from other media (books, games, TV shows, toys, comics, etc.)\n",
    "- is_superhero: Features superheroes as main characters\n",
    "- has_remake_indicator: Title contains remake-related words\n",
    "- is_remake_adaptation: Any type of remake or adaptation\n",
    "- is_ip_movie: Based on existing intellectual property (comprehensive IP flag)\n",
    "\n",
    "**Instructions:**\n",
    "1. Use your knowledge of this specific movie to verify each feature\n",
    "2. Focus on accuracy - only return corrections for features that are definitively wrong\n",
    "3. Pay special attention to:\n",
    "   - Studio/distributor accuracy (which studio actually distributed this movie?)\n",
    "   - IP classification (Marvel, DC, adaptations, remakes, sequels)\n",
    "   - Franchise identification (is this really part of these franchises?)\n",
    "   - Origin country information (where was this movie actually made?)\n",
    "   - Language (is this actually in English?)\n",
    "4. Provide your response as a JSON object with only the corrected features\n",
    "5. If all features are correct, return: {{\"status\": \"all_correct\"}}\n",
    "6. For corrections, include a brief but clear reason\n",
    "\n",
    "**Response Format:**\n",
    "```json\n",
    "{{\n",
    "  \"corrections\": {{\n",
    "    \"feature_name\": {{\n",
    "      \"old_value\": false,\n",
    "      \"new_value\": true,\n",
    "      \"reason\": \"Brief explanation of why this is correct\"\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "Please provide your analysis:\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def validate_movie_features(movie_data, client, model=\"gpt-4o\", max_retries=3):\n",
    "    \"\"\"\n",
    "    Send movie data to GPT-4o for feature validation\n",
    "    \"\"\"\n",
    "    prompt = create_validation_prompt(movie_data)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a movie expert who validates feature classifications for machine learning models. Always respond with valid JSON. Focus on accuracy and only correct features that are definitively wrong.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=2000  # Increased for all 34 features\n",
    "            )\n",
    "            \n",
    "            # Extract JSON from response\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Remove markdown code block markers if present\n",
    "            if content.startswith('```json'):\n",
    "                content = content[7:]\n",
    "            if content.endswith('```'):\n",
    "                content = content[:-3]\n",
    "            \n",
    "            # Parse JSON response\n",
    "            result = json.loads(content)\n",
    "            return result\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error for {movie_data['title']} (attempt {attempt + 1}): {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\"error\": f\"JSON decode failed after {max_retries} attempts\"}\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"API error for {movie_data['title']} (attempt {attempt + 1}): {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\"error\": f\"API call failed after {max_retries} attempts: {str(e)}\"}\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return {\"error\": \"Max retries exceeded\"}\n",
    "\n",
    "print(\"Validation functions defined with ALL 34 specified features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Test Validation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "try:\n",
    "    client = openai.OpenAI()\n",
    "    print(\"OpenAI client initialized successfully\")\n",
    "    \n",
    "    # Test with a sample movie\n",
    "    test_movie = df[df['title'].str.contains('Mario', case=False, na=False)].iloc[0] if len(df[df['title'].str.contains('Mario', case=False, na=False)]) > 0 else df.iloc[0]\n",
    "    \n",
    "    print(f\"\\nTesting validation system with: {test_movie['title']} ({test_movie['release_year']})\")\n",
    "    \n",
    "    # Run validation\n",
    "    result = validate_movie_features(test_movie, client)\n",
    "    \n",
    "    print(\"\\nValidation result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    print(\"Please make sure you have set your OpenAI API key:\")\n",
    "    print(\"1. Set environment variable: export OPENAI_API_KEY='your-key-here'\")\n",
    "    print(\"2. Or uncomment and set the api_key in the first cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Batch Validation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_corrections(df, corrections_log):\n",
    "    \"\"\"\n",
    "    Apply all collected corrections to the dataframe\n",
    "    \"\"\"\n",
    "    corrected_df = df.copy()\n",
    "    total_corrections = 0\n",
    "    \n",
    "    for idx, corrections in corrections_log.items():\n",
    "        if 'corrections' in corrections:\n",
    "            for feature, correction_info in corrections['corrections'].items():\n",
    "                old_value = correction_info['old_value']\n",
    "                new_value = correction_info['new_value']\n",
    "                reason = correction_info['reason']\n",
    "                \n",
    "                # Apply correction\n",
    "                corrected_df.at[idx, feature] = new_value\n",
    "                total_corrections += 1\n",
    "                \n",
    "                print(f\"Row {idx} ({corrected_df.at[idx, 'title']}): {feature} {old_value} ‚Üí {new_value} ({reason})\")\n",
    "    \n",
    "    print(f\"\\nTotal corrections applied: {total_corrections}\")\n",
    "    return corrected_df\n",
    "\n",
    "def validate_dataset_subset(df, start_idx=0, end_idx=None, save_progress=True):\n",
    "    \"\"\"\n",
    "    Validate a subset of the dataset\n",
    "    \"\"\"\n",
    "    if end_idx is None:\n",
    "        end_idx = len(df)\n",
    "    \n",
    "    subset = df.iloc[start_idx:end_idx]\n",
    "    corrections_log = {}\n",
    "    errors_log = {}\n",
    "    \n",
    "    print(f\"Validating movies {start_idx} to {end_idx-1} ({len(subset)} movies)\")\n",
    "    \n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize OpenAI client: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    for idx, row in tqdm(subset.iterrows(), total=len(subset), desc=\"Validating movies\"):\n",
    "        try:\n",
    "            # Validate movie features\n",
    "            result = validate_movie_features(row, client)\n",
    "            \n",
    "            # Store result\n",
    "            if 'error' in result:\n",
    "                errors_log[idx] = result\n",
    "            else:\n",
    "                corrections_log[idx] = result\n",
    "            \n",
    "            # Rate limiting - wait between API calls\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            # Save progress every 10 movies\n",
    "            if save_progress and (idx - start_idx) % 10 == 9:\n",
    "                progress_file = f'../data/validation_progress_{start_idx}_{end_idx}.json'\n",
    "                with open(progress_file, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'corrections': corrections_log,\n",
    "                        'errors': errors_log,\n",
    "                        'last_processed': idx\n",
    "                    }, f, indent=2)\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\nValidation interrupted at row {idx}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error for row {idx}: {e}\")\n",
    "            errors_log[idx] = {\"error\": str(e)}\n",
    "    \n",
    "    return corrections_log, errors_log, subset\n",
    "\n",
    "print(\"Batch validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Select Movies for Validation\n",
    "\n",
    "Let's prioritize validation for movies that are most likely to have errors, especially 2026 movies and IP-related films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prioritize movies for validation\n",
    "print(\"Selecting movies for validation...\")\n",
    "\n",
    "# High priority: 2026 movies (most likely to have errors)\n",
    "movies_2026 = df[df['release_year'] == 2026].copy()\n",
    "print(f\"2026 movies: {len(movies_2026)}\")\n",
    "\n",
    "# Medium priority: Recent movies (2024-2025) with IP/adaptation flags\n",
    "recent_ip_movies = df[\n",
    "    (df['release_year'].isin([2024, 2025])) & \n",
    "    (df['is_ip_movie'] == 1)\n",
    "].copy()\n",
    "print(f\"Recent IP movies (2024-2025): {len(recent_ip_movies)}\")\n",
    "\n",
    "# Sample of other movies for general validation\n",
    "other_movies = df[\n",
    "    (~df.index.isin(movies_2026.index)) & \n",
    "    (~df.index.isin(recent_ip_movies.index))\n",
    "].sample(n=min(50, len(df) - len(movies_2026) - len(recent_ip_movies)), random_state=42)\n",
    "print(f\"Sample of other movies: {len(other_movies)}\")\n",
    "\n",
    "# Combine prioritized datasets\n",
    "validation_subset = pd.concat([movies_2026, recent_ip_movies, other_movies]).reset_index(drop=True)\n",
    "print(f\"\\nTotal movies selected for validation: {len(validation_subset)}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of movies to validate:\")\n",
    "display(validation_subset[['title', 'release_year', 'is_ip_movie', 'is_adaptation', 'is_marvel', 'is_dc']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Run Validation (Uncomment to Execute)\n",
    "\n",
    "**Warning**: This will make API calls to OpenAI. Make sure you have:\n",
    "1. Set your OpenAI API key\n",
    "2. Sufficient API credits\n",
    "3. Are ready to process the selected movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to run the validation\n",
    "# This will make API calls to OpenAI\n",
    "\n",
    "# print(\"Starting validation process...\")\n",
    "# print(\"This may take several minutes depending on the number of movies.\")\n",
    "# print(f\"Estimated cost: ~${len(validation_subset) * 0.01:.2f} (rough estimate)\")\n",
    "# \n",
    "# # Run validation on the prioritized subset\n",
    "# corrections_log, errors_log, validated_subset = validate_dataset_subset(\n",
    "#     validation_subset, \n",
    "#     start_idx=0, \n",
    "#     end_idx=len(validation_subset)\n",
    "# )\n",
    "# \n",
    "# # Display results\n",
    "# if corrections_log is not None:\n",
    "#     print(f\"\\nValidation completed!\")\n",
    "#     print(f\"Movies processed: {len(validated_subset)}\")\n",
    "#     print(f\"Movies with corrections: {len([c for c in corrections_log.values() if 'corrections' in c])}\")\n",
    "#     print(f\"Errors encountered: {len(errors_log)}\")\n",
    "#     \n",
    "#     # Save validation results\n",
    "#     results_file = '../data/llm_validation_results.json'\n",
    "#     with open(results_file, 'w') as f:\n",
    "#         json.dump({\n",
    "#             'corrections': corrections_log,\n",
    "#             'errors': errors_log,\n",
    "#             'validation_metadata': {\n",
    "#                 'total_movies': len(validated_subset),\n",
    "#                 'validation_date': pd.Timestamp.now().isoformat(),\n",
    "#                 'model_used': 'gpt-4o'\n",
    "#             }\n",
    "#         }, f, indent=2)\n",
    "#     \n",
    "#     print(f\"Results saved to: {results_file}\")\n",
    "# else:\n",
    "#     print(\"Validation failed to initialize\")\n",
    "\n",
    "print(\"Validation code ready (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Apply Corrections and Rebuild Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and apply validation results (uncomment when you have results)\n",
    "\n",
    "# try:\n",
    "#     # Load validation results\n",
    "#     with open('../data/llm_validation_results.json', 'r') as f:\n",
    "#         validation_results = json.load(f)\n",
    "#     \n",
    "#     corrections_log = validation_results['corrections']\n",
    "#     errors_log = validation_results['errors']\n",
    "#     \n",
    "#     print(f\"Loaded validation results:\")\n",
    "#     print(f\"- Movies with corrections: {len([c for c in corrections_log.values() if 'corrections' in c])}\")\n",
    "#     print(f\"- Errors: {len(errors_log)}\")\n",
    "#     \n",
    "#     # Apply corrections to the full dataset\n",
    "#     print(\"\\nApplying corrections to dataset...\")\n",
    "#     corrected_df = apply_corrections(df, corrections_log)\n",
    "#     \n",
    "#     # Save corrected dataset\n",
    "#     corrected_path = '../data/dataset_domestic_processed_llm_validated.csv'\n",
    "#     corrected_df.to_csv(corrected_path, index=False)\n",
    "#     print(f\"\\nCorrected dataset saved to: {corrected_path}\")\n",
    "#     \n",
    "#     # Generate validation report\n",
    "#     print(\"\\n=== VALIDATION REPORT ===\")\n",
    "#     \n",
    "#     # Count corrections by feature\n",
    "#     feature_corrections = {}\n",
    "#     for idx, corrections in corrections_log.items():\n",
    "#         if 'corrections' in corrections:\n",
    "#             for feature in corrections['corrections'].keys():\n",
    "#                 feature_corrections[feature] = feature_corrections.get(feature, 0) + 1\n",
    "#     \n",
    "#     print(\"\\nMost frequently corrected features:\")\n",
    "#     for feature, count in sorted(feature_corrections.items(), key=lambda x: x[1], reverse=True):\n",
    "#         print(f\"  {feature}: {count} corrections\")\n",
    "#     \n",
    "#     # Analyze improvement in key features\n",
    "#     key_features = ['is_ip_movie', 'is_adaptation', 'is_marvel', 'is_dc', 'is_superhero', 'is_sequel']\n",
    "#     \n",
    "#     print(\"\\nFeature accuracy improvements:\")\n",
    "#     for feature in key_features:\n",
    "#         if feature in corrected_df.columns:\n",
    "#             original_count = df[feature].sum()\n",
    "#             corrected_count = corrected_df[feature].sum()\n",
    "#             change = corrected_count - original_count\n",
    "#             print(f\"  {feature}: {original_count} ‚Üí {corrected_count} ({change:+d})\")\n",
    "#     \n",
    "# except FileNotFoundError:\n",
    "#     print(\"No validation results found. Run validation first.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading validation results: {e}\")\n",
    "\n",
    "print(\"Correction application code ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook provides a complete system for validating and correcting movie features using GPT-4o. To use it:\n",
    "\n",
    "1. **Set up API access**: Add your OpenAI API key\n",
    "2. **Test the system**: Run the test validation to ensure everything works\n",
    "3. **Run validation**: Uncomment the validation code to process your selected movies\n",
    "4. **Apply corrections**: Use the results to generate a corrected dataset\n",
    "5. **Rebuild features**: Use the corrected dataset for model training\n",
    "\n",
    "The system prioritizes:\n",
    "- 2026 movies (most likely to have errors)\n",
    "- Recent IP movies (2024-2025)\n",
    "- A random sample of other movies for general validation\n",
    "\n",
    "Key features validated:\n",
    "- IP movie classification\n",
    "- Adaptation detection\n",
    "- Franchise identification (Marvel, DC, Star Wars, etc.)\n",
    "- Sequel detection\n",
    "- Studio classification\n",
    "- Superhero movie identification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "box_office",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
